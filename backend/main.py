from fastapi import FastAPI
from pydantic import BaseModel
from backend.retrieval.retriever import Retriever

# load your retriever
retriever = Retriever()

# FastAPI app
app = FastAPI()

class Query(BaseModel):
    question: str

@app.post("/ask")
async def ask_rag(query: Query):
    context_docs = retriever.search(query.question)

    # Combine the documents into a single context string
    context = "\n\n".join([doc.page_content for doc in context_docs])

    # YOUR OPEN-SOURCE MODEL (Ollama or HuggingFace)
    # We'll replace this once you tell me your preferred model.
    # For now, simple demo response:
    answer = f"Context:\n{context}\n\nYour answer will be generated by your chosen LLM."

    return {
        "question": query.question,
        "context_used": context_docs,
        "answer": answer
    }
